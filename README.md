# DesignPattern

这里记录了 「设计模式」 的学习记录

目前记录到了抽象工厂模式

单例模式

工厂模式

原型模式

建造者模式

适配器模式

桥接模式

大数据 ACP

考试大纲
题目类型：共100道题，每题分值1分，单选:多选: 判断= 5 : 3 : 2
知识点	试题比例
大数据基础知识	10%
大数据存储技术	10%
数据分析工具	10%
数据可视化	10%
数据编程	25%
数据项目质量控制	15%
数据项目设计与执行	10%
机器学习	10%

第1章  大数据基础知识
1.1 大数据基础
1、大数据的概念：
大数据（Big Data)，是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合。是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。
2、大数据的特点：
概括为4V特征：Volume体量巨大、Variety种类繁多、Velocity处理速度快、value价值密度低（即单位体量数据包含的价值相对较低）。【重点】
3、云计算与大数据：
云计算就是一种提供资源的网络，使用者可以随时获取“云”上的资源（比如计算资源、存储资源），按需求量使用，并且可以看成是无限扩展的。
云计算是一种服务模式，从技术上讲是一种分布式计算模式。而大数据的目的主要是通过海量数据发现潜在价值，使人们更好的理解和把握信息。
云计算与大数据的区别和联系主要有以下几点：【重点】
✓大数据是为了发掘信息价值，而云计算主要是通过互联网管理资源，提供相应的服务。
✓大数据的价值在于发掘数据的有效信息，云计算则可以大量节约使用存储成本。
✓云计算为大数据应用提供基础的硬件设施
1.2 数据类型
1、数据类型通常分为如下三种【重点】
结构化数据：有固定的结构，严格地遵循数据格式与长度规范。一般这样的数据来自业务系统的关系型数据库。
半结构化数据：是有结构的数据，但是结构是不固定的，变化很大。比如xml、json个数的数据。
非结构化数据：没有固定结构的数据。各种文档、图片、视频/音频等都属于非结构化数据。
2、不同类型数据分析处理方法
结构化数据：可以直接用大数据平台Hive、阿里云MaxCompute来存储和处理。
半结构化数据：一般这样的数据来自日志和网络。可以用大数据平台Hive、阿里云MaxCompute、NoSQL数据库来存储。注意要解析原始的半结构化数据并抽取出结构化信息。
非结构化数据：一般来自网络和设备数据。可以用分布式文件系统HDFS、阿里云对象存储OSS、MongoDB等来存储。不同格式数据需要专门解析程序解析处理。
1.3 大数据相关技术
1、开源大数据技术
HDFS：分布式文件系统，是hadoop体系中数据存储管理的基础，核心存储框架。【重点】
MapReduce：mapreduce是一种计算模型，用于处理大数据量的计算。它借助于函数式程序设计语言Lisp的设计思想，提供了一种简便的并行程序设计方法。用Map（映射）和Reduce（归约）两个函数编程实现基本的并行计算任务。先进行Map运算，得出Key-Value的中间结果，然后Reduce对中间结果中相同的键的所有值进行归约，以得到最终结果。【重点】
Yarn：是一种新的Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度。【重点】
Hive：数据仓库工具。hive定义了一种类似sql的查询语言（hql）将sql转化为mapreduce任务在hadoop上执行。【重点】
HBase：hbase是一个针对结构化数据的可伸缩，高可靠，高性能，分布式和面向列的动态模式数据库。hbase采用了bigtable的数据模型。
Store：storm是一个分布式的，容错的计算系统，storm属于流处理平台。
Impala: cloudrea开发的实时交互SQL大数据查询引擎。适用于即席查询。
2、阿里云大数据平台产品体系
阿里云大数据体系由多个大数据产品组件组成，主要产品包括：
大数据计算服务MaxCompute：快速、完全托管的TB/PB级数据仓库解决方案。MaxCompute适合海量数据的离线计算；不适合做即席查询以及业务系统的数据存储。
数据工场DataWorks：是大数开发、数据管理、数据治理平台。提供数据集成、数据开发、数据地图、数据质量和数据服务等功能。
数据可视化分析平台QuickBI：提供海量数据实时在线分析服务，通过拖拽式操作和丰富的可视化效果，进行数据分析、业务数据探查、报表制作等工作。
DataV数据可视化：可视化大屏工具。提供丰富场景模板和图形化组件，无需专业编程人员也可快速实现专业水准的可视化大屏数据展现。
机器学习平台PAI：PAI（Platform of Artificial Intelligence）：是构建在阿里云MaxCompute计算平台之上，集数据处理、建模、离线预测、在线预测为一体的机器学习平台。为算法开发者提供了丰富的MPI、PS、BSP等编程框架和数据存储接口，同时提供了基于WEB的可视化控制台，降低了使用门槛。
实时计算Flink版：是阿里云基于Apache Flink构建的企业级、高性能实时大数据处理系统。
交互式分析Hologres：是实时交互式分析产品，兼容PostgreSQL协议，与大数据生态无缝连接，支持高并发和低延时地查询分析万亿级数据，帮助您轻松的使用现有BI工具分析业务数据。
云原生数仓AnalyticDB：云原生数据仓库AnalyticDB MySQL版（简称ADB，原分析型数据库MySQL版），是阿里巴巴自主研发的海量数据实时高并发在线分析云计算服务，使得您可以在毫秒级针对千亿级数据进行即时的多维分析透视和业务探索。适合做大数据交互式分析。
3、什么是数据分析
数据分析基于商业目的，有目的的进行收集、整理、加工和分析数据，提炼有价值信息的过程。
4、数据分析过程
数据分析一般包括如下过程：明确目的、数据收集、数据处理、数据分析、数据展现、分析报告。【重点】
明确目的：明确数据分析的目标、数据指标，以及要达到的效果
数据收集：确定数据范围、获取目标数据、整合相关数据。不同阶段根据需求采集所需数据范围和数据类型，同时要考虑数据质量。数据源多了，往往需要对不同的数据源做不同的数据预处理，有时候数据多样性还会降低数据质量。【重点】
数据处理：包括处理缺失数据、清洗不一致数据、关联、汇总数据
数据分析：通过SQL、编程等实现数据分析
数据展现：将数据分析结果以图表、报表形式直观展现
分析报告：简单可靠、清晰明了、逻辑性强的数据分析报告
第2章  数据存储技术
2.1 分布式存储
1、分布式存储
分布式存储是一种数据存储技术，通过网络使用企业中的每台机器上的磁盘空间，并将这些分散的存储资源构成一个虚拟的存储设备，数据分散的存储在企业的各个角落。（via百度百科）

分布式存储系统，是将数据分散存储在多台独立的设备上。传统的网络存储系统采用集中的存储服务器存放所有数据。

2、分布式的特点：可扩展、低成本、高性能、易用
3、分布式存储系统的分类：
分布式文件系统：存储非结构化数据对象，作为其它存储系统的底层存储，可以存储三种类型的数据——Blob对象、定长块、大文件，分布式文件系统内部按照数据块来组织数据，将数据块分散到存储集群，处理数据复制、一致性、负载均衡、容错等问题，如HDFS
分布式键值系统：存储关系简单的半结构化数据，支持数据分布到集群中的多个存储节点，一致性哈希就是分布式键值系统中常用的数据分布技术
分布式表格系统：存储关系较为复杂的半结构化数据，以表格为单位组织数据，支持主键CRUD功能以及范围查找功能，针对单张表格操作，同一个表格的多个数据行不要求包含相同类型的列，可以做到超大规模，支持较多的功能
分布式数据库：存储结构化数据，采用二维表格组织数据，提供SQL关系查询语言；如DRDS
4、分布式文件系统HDFS：
一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析。【重点】
5、HDFS主要由四个部分组成
分别为HDFS Client、NameNode、DataNode 和Secondary NameNode，为了保证数据安全，每个存储在HDFS 上的数据文件，可以设置不同的备份数，一般至少3份。【重点】
6、常见的数据存储方式：
传统的关系型数据库：Oracle、MySQL；行存，适合OLTP业务
NoSQL数据库：HBase、Cassandra、Redis；列存，适合OLAP业务
2.3 数据库基础知识
1. 数据库的概念：
   数据库是以一定方式储存在一起、多用户共享、具有尽可能小的冗余度、与应用程序彼此独立的数据集合；数据库管理系统（Database Management System，DBMS）是为管理数据库而设计的软件系统，用于建立、使用和维护数据库。
   传统数据库系统分成网状数据库（Network database）、层次数据库（Hierarchical database）和关系数据库（Relational database）三类。【重点】
2. 数据库的常见概念：
   表：数据库中组织存储数据的单位，由行和列组成；
   主键、外键：一张表中，可以用于唯一标识一条记录的字段组（或者说是属性组），主键保证了数据的唯一性；外键用于与另一张表的关联。是能确定另一张表记录的字段，用于保持数据的一致性。比如，A表中的一个字段，是B表的主键，那他就可以是A表的外键；外键保证了数据的完整性；【重点】
   行式存储和列式存储：存储时组织表的方式不同，按照行顺序或者按照列顺序。传统关系型数据库通常使用行式存储；【重点】
   事务（Transaction）是一系列在共享数据库上执行的行为，以达到更高层次更复杂逻辑的功能。事务是DBMS中最基础的单位，不可分割。
   索引（Index）是对数据库表中一列或多列的值进行排序的一种结构，使用索引可快速访问数据库表中的特定信息。
3. 数据库的三种约束：【重点】
   实体完整性约束指的是主键不能为空
   参照完整性约束，即外键的约束，某一外键的值必须在它引用的主键字段中存在。如，学生表中专业编号属性的值，必须都存于专业信息表中的专业编号属性中
   用户自定义完整性约束，指的是一些用户自己设定的约束，例如字段是否可以为空，字段值的取值范围（如：人的性别只能取男、女）
   2.4 数据仓库基本知识
   1、数据仓库的基本概念：
   数据仓库（Data Warehouse）是一个面向主题的（Subject Oriented）、集成的（Integrated）、非易失的（Non-Volatile）、反映历史变化（Time Variant）的数据集合，用于支持管理决策(Decision Making Support)。【重点】
   2、常见概念：
   维度（Demention）：是指人们观察事物的角度,是指一种视角，是一个判断、说明、评价和确定一个事物的多方位、多角度、多层次的条件和概念，如时间维度、地区维度、产品维度等
   维度表：维度属性的集合，分析数据的窗口
   事实表：记录业务数据的实体
   粒度：是指数据仓库的数据单位中保存数据的细化或综合程度的级别；细化程度越高，粒度级就越小；相反，细化程度越低，粒度级就越大
   联机事务处理（On-Line Transaction Processing，OLTP）：是指事务性非常高的系统，一般都是高可用的在线系统，以小事务以及小查询为主，评估其系统时，一般看其每秒执行的Transaction 以及Execute SQL的数量【重点】
   联机分析处理（On-Line Analytical Processing，OLAP）：是数据仓库系统最主要的应用，专门设计用于支持复杂的分析操作，侧重对决策人员和高层管理人员的决策支持，可以根据要求快速、灵活地进行大数据量的复杂查询处理，并以一种直观易懂的形式将查询结果提供给决策人员，以便他们准确掌握企业的经营状况，了解对象的需求，制定正确的方案。【重点】
   数据立方体（data  cube）：是一种面向“主题”和“属性”而建立起来的一类多维矩阵，让用户从多个角度探索和分析数据集，通常是一次同时考虑三个因素（维度）;但是数据立方体不局限于三个维度，大多数在线分析处理（OLAP）系统能用很多个维度构建数据立方体；联机分析处理的常见操作：切片、切块、旋转（转轴）【重点】
   ETL：是Extract、Transform、Loading三个字母的缩写，即数据抽取、转换、装载。将数据从原始业务中抽取出来是所有工作的前提，必须在业务系统运行时对数据进行实时或者准实时的提取或者是定时的批量抽取。【重点】
   3、数据仓库和数据库的区别【重点】

2.5 Hadoop与MaxCompute
Hadoop 是一个分布式系统基础架构，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。
核心组件是HDFS和MapReduce：HDFS为海量的数据提供了分布式存储，则MapReduce为海量的数据提供了计算；适用场景：海量数据（结构化、非结构化数据）分布式计算、存储。【重点】
MaxCompute是一种快速、完全托管的EB级数据仓库解决方案。基于阿里云飞天架构和分布式技术，底层存储系统为盘古（类似hdfs）;组织逻辑单元为Project，数据存储单元为表（Table），表可以设置分区，可以按天设置生命周期；适用场景：结构化海量数据计算和存储。【重点】
二者皆不适宜OLTP系统以及数据的实时查询、在线分析场景。
第3章  数据分析工具
3.1 MapReduce的基本概念与特点
1、什么是MapReduce
MapReduce是Hadoop 的三大组件之一，是Hadoop提供的基于HDFS的计算框架。基于这个框架，够容易地编写应用程序，这些应用程序能够运行在由上千个商用机器组成的大集群上，并以一种可靠的，具有容错能力的方式并行地处理上TB级别的海量数据集。
MapReduce的基本思想就是分而治之。
2、MapReduce基本原理
MapReduce把一个数据处理过程分成Map和Reduce两个阶段：
第1阶段、是Map阶段，把复杂的任务分解为若干个并发的“简单的任务”来处理
第2阶段，是Reduce阶段，对map阶段的结果进行汇总
3、MapReduce的特点：
适合海量数据批处理
适合处理半结构和非结构化数据
性能可随机器数量线性扩展
适用多种应用场景，如Web访问日志分析
可应用于机器学习的复杂算法，（比如：Mhout，就基于mapreduce实现的机器学习算法库）。
3.2 MySQL、MaxCompute、Hive
MySQL：小规模一般统计分析可以使用MySQL来实现。通过客户端工具将数据导入到MySQL，使用SQL语言及MySQL内置函数实现数据清洗、加工、统计。
MaxCompute：适合海量数据离线分析。通过数据集成、本地客户端等工具将业务数据同步到MaxCompute。通过DataWorks开发业务流程，使用MaxComputeSQL完成数据清洗、加工、统计。结合机器学习平台PAI，进行机器学习模型训练。
Hive：适合海量数据离线分析。通过Sqoop等工具将数据库数据导入Hive，通过HiveSQL完成数据清洗、加工、统计。
3.3 常见数据分析工具介绍
1. Excel
   Excel对数据进行统计分析展示提供了模拟运算表、单变量求解、规划求解、方案管理器等丰富的数据分析工具，Excel技术门槛低，上手快，无需编程。常用操作有函数、可视化、数据透视表、VBA。但Excel也有如下劣势，使用时应注意：
   Excel不能对用户进行角色管理，它的保密性较差，容易被其他软件破解【重点】
   Excel不能处理大数据，当数据量过大的时候，查询和计算速度会下降，拉低办公效率【重点】
   Excel做图形展现，可能会花费很多时间编辑图表，包括颜色、字体的设定等【重点】
2. R
   R是一个全面的统计研究平台，提供了各式各样的数据分析技术。几乎任何类型的数据分析工作皆可在R中完成。
   R是用于统计学计算和绘图的语言。最早是分析师常用的编码，但是R作为开源的项目，有很多人做了扩展包，可以使得统计绘图和分析更加简单。比如做聚类分析可以使用mclust包。【重点】
3. python
   Python 是一种解释型、交互式、面向对象、动态数据类型的高级程序设计语言。
   用Python进行数据分析时常用包有numpy、scipy、matplotlib等，可以进行各种数据分析，比如聚类分析。【重点】
4. 阿里云机器学习平台PAI
   机器学习PAI（Platform of Artificial Intelligence）是阿里云人工智能平台，提供一站式的机器学习解决方案。
   PAI支持丰富的机器学习算法、一站式的机器学习体验、主流的机器学习框架及可视化的建模方式。
   PAI的算法都经过阿里巴巴集团大规模业务的沉淀，不仅支持基础的聚类和回归类算法，同时也支持文本分析和特征处理等复杂算法。【重点】
   3.4 SQL基础知识
   SQL (Structured Query Language:结构化查询语言) 是用于管理关系数据库管理系统（RDBMS）。SQL 的范围包括数据插入、查询、更新和删除，数据库模式创建和修改，以及数据访问控制。
   关系型数据库管理系统
   关系数据库管理系统（RDBMS）全称Relational Database Management System。
   RDBMS 是SQL 的基础，同样也是所有现代数据库系统的基础，比如MS SQL Server、IBM DB2、Oracle、MySQL 以及Microsoft Access。
   RDBMS 中的数据存储在被称为表的数据库对象中。表是相关的数据项的集合，它由列（字段）和行（记录）组成。
   关系数据库使用主外键机制维护维护关系数据库的完整性。主键是能确定表中一条记录的唯一标识。外键用于约束与另一张表的关联关系。删除主表记录时，如果主表主键作为了其它表的外键来使用，则需要先解除外键约束，否则主表记录不能删除。【重点】
   数据库管理数据的优势 【重点】
   相比于文件系统，文件系统不能解决数据冗余和数据独立性问题，而数据库系统可以解决
   相比于文件系统，数据库系统查询数据更加方便，并且解决了数据的一致性维护问题
   DDL
   用于定义数据库的结构，包括CREATE、ALTER、DROP、TRUNCATE、COMMENT、RENAME等语句。
   DML
   DML实现对数据库中数据的操作，包括SELECT、INSERT、UPDATE、DELETE等。
   SELECT语句
   SELECT 语句用于从数据库中选取数据。提供了字段运算、数据筛选、分组聚合、多表关联等功能。
   语句语法简单归纳为：
   SELECT select_list
   [FROM table_source]
   [WHERE search_condition]
   [GROUP BY group_by_expression]
   [HAVING search_condition]
   [ORDER BY order_expression [ASC | DESC]]
   where子句
   ✓SQL使用where子句对from表中的数据过滤，只取符合where子句表达式的纪录。比如select * from t1 where id=1，只返回表t1中字段id值为1的记录。【重点】
   ✓常用的筛选方法
   	IN：希望从表customers中查询出城市为"北京"，"上海"，"广州"的客户信息，可以这样写：SELECT * FROM customers WHERE city IN ('北京','上海','广州')。另外IN的列表可以是一个SELECT子查询。【重点】
   	Like：SQL LIKE 子句中使用百分号%字符来表示任意字符，比如select * from t1 where area like '中%’，以“中”开头的记录都返回。【重点】
   表连接的常用三种方式【重点】
   ✓INNER JOIN（内连接,或等值连接）：获取两个表中字段匹配关系的记录。
   ✓LEFT JOIN（左连接）：获取左表所有记录，即使右表没有对应匹配的记录。右表不匹配的记录，右表的字段返回NULL。
   ✓RIGHT JOIN（右连接）：与LEFT JOIN 相反，用于获取右表所有记录，即使左表没有对应匹配的记录。左表不匹配的记录，左表的字段返回NULL。
   ✓由于join是笛卡尔乘积计算，使用join时要注意连接字段有重复记录时，返回的记录数可能会多于输入表。
   去除重复数据distinct
   distinct用来去除查询结果中的重复记录。【重点】

内置函数
SQL常用内置函数有字符串函数，数学函数、日期函数等。比如：计算表t1中一个字段val1的平均值，可以用selectavg(val1) from t1来实现。【重点】

3.5 MaxCompute SQL
1. MaxComputeSQL 基本概念
   MaxComputeSQL是对标准SQL的语义支持，采用的是类似于关系数据库SQL的语法，但不能因此简单地把MaxCompute等价成一个数据库，它在很多方面并不具备数据库的特征，如事务、主键约束、索引等。
2. MaxComputeSQL DDL操作
   1）DDL操作语句包括：create、alter、drop（删除分区表或非分区表）、desc、show。
   create: 创建表、分区
   alter:修改表结构，包括字段、分区、生命周期
   drop：删除表、分区
   truncate：清空非分区表数据，注意，清除分区表中数据用drop partition
   desc：查看表结构、表占用空间大小等
   show：查看表建表语句、列出表、视图、分区
   详细语法和示例见官方文档：https://help.aliyun.com/document_detail/74236.html
3. MaxComputeSQL DML操作（Insert）
   1）详细语法见官方文档：https://help.aliyun.com/document_detail/74250.html
   2）注意事项
   支持多路输出，即在一个语句中实现读取一次写入多个目标表的操作
   进行insert 更新数据操作时，源表与目标表的对应关系依赖于在select 子句中列的顺序，而不是表与表之间列名的对应关系
   向某个具体的分区插入数据时，分区列不允许出现在select 列表中
4. MaxComputeSQL DQL操作（SELECT）【重点】
   1）查询数据select，详细语法见官方文档：
   https://help.aliyun.com/document_detail/74250.html
   2）注意事项
   在选取的列名前可以使用distinct去掉重复字段。
   支持多表的连接（join），包括left join、right join、innerjoin、full join。
   注意半连接left semi join和left anti join，右表只作为过滤条件，右表的数据不出现在结
   果集中。
   join使用限制：
   ✓关联条件需为等值连接，不能使用非等值连接
   ✓多个条件之间需用and连接，不能使用or连接的关联条件
5. MaxCompute支持多种内置函数，包括：
   日期函数。例如：selectdatediff（date1,date2）fromtable1，计算两个时间date1、date2的差值。
   数学函数，例如：selectabs(val1) from table1，取绝对值。
   窗口函数
   聚合函数，例如：selectavg(val1) from table1，取平均值。
   字符串函数
   其他函数 详见官方文档：https://help.aliyun.com/document_detail/48969.html
   3.6 DataWorks基本使用
1. DataWorks功能介绍
   数据集成：数据集成是稳定高效、弹性伸缩的数据同步平台，致力于提供复杂网络环境下、丰富的异构数据源之间高速稳定的数据移动及同步能力。
   数据开发：数据开发，就是完成在大数据平台上的数据处理、数据计算的程序、流程的开发。DataWorks提供了一个基于Web的可视化集成开发环境，通过拖拽、页面配置的方式，进行数据处理流程的开发。
   任务运维：主要是实现离线任务的运维管理，包括运维监控、运维操作、告警等功能。
   数据治理：包括数据地图、数据质量、安全中心、数据保护伞几方面功能。
   数据服务：数据服务就是把数据平台统计分析的结果、报表，封装成服务API，进行数据分享、数据变现。
   数据应用：包括数据分析和应用开发，应用开发提供了App Studio，全托管的Web应用开发工具。
2. DataWorks使用流程
   DataWorks服务开通整体流程步骤：注册账号、购买MaxCompute资源；创建工作空间；创建子账号、分配项目管理员权限；添加项目组成员。
   DataWorks开发流程的几个步骤：数据输入、数据加工、数据输出、代码发布、生产调度、生产运维。
3. DataWorks建表
   通过创建临时查询，执行createtable语句建表
   通过表管理，可视化建表
4. DataWorks数据上传
   DataWorks数据集成，实现各类型数据上传到MaxCompute
   DataWorks表管理，提供了本地数据上传功能

第4章  数据可视化
4.1 数据可视化的基本知识
可视化的目标、本质：
可视化的本质是形象、直观地表达数据蕴含的信息和规律；因此判定好图表的标准重点考察数据方面；数据是否与受众的兴趣点强烈相关，数据内容之间是否有逻辑连贯性，然后才是视觉效果问题、美化问题、数据全面问题
4.2 阿里云数据可视化平台Quick BI
QuickBI：提供海量数据实时在线分析服务，支持拖拽式操作和丰富的可视化效果，帮助您轻松自如地完成数据分析、业务数据探查、报表制作等工作。Quick BI不仅是业务人员查看数据的工具，更是数据化运营的助推器。其优势特点：【重点】
快速搭建数据门户、开发数据报表、数据建模
智能数据分析和交互
安全管控数据权限
丰富灵活的数据可视化分析
4.3 常见图表类型的特点和适用场景
1、折线图特点：【重点】
场景：数据在一个有序的因变量上的变化，它的特点是反映事物随序类别而变化的趋势，可以清晰展现数据的增减趋势、增减的速率、增减的规律、峰值等特征
优点：
✓能很好的展现沿某个维度的变化趋势
✓能比较多组数据在同一个维度上的趋势
✓适合展现较大的数据集
类似图表：堆积图、曲线图、双Y轴折线图、面积图
2、柱图特点：【重点】
场景：适合用于展示二维数据集，其中一个轴表示需要对比的分类维度
优点：
✓易于比较各组数据之间的差别
类似图表：条形图、直方图、堆积图、百分比堆积图、双Y轴等
3、分析类型与图表对应关系 不同的数据需要不同的图表类型来展示，常见分析类型如下：【重点】
趋势分析（维度变化）：组合图、线图、面积图、堆积面积图、百分比堆叠面积图
分布分析（占比分析）：饼图、雷达图、玫瑰图、矩阵树图、词云图
比较分析：柱图、堆积柱状图、百分比堆积柱状图、条形图、堆积条形图、百分比堆积条形图

4.4可视化产品的设计原则
可视化的设计原则：【重点】
突出变化：快速掌握业务的变化，要将这个变化放在最突出醒目的位置，同时要考虑KPI
的时效性
引发问题：KPI数值的变化能够直接导致业务部门提出问题
保持一致：帮助观众在理解KPI背后的故事的时候不易发生歧义
美观易懂：包括颜色的使用，各个模块和图表的位置、大小和他们之间的逻辑关系等等
第5章  数据编程
5.1 数据预处理
数据集成：针对来自不同数据源的数据，进行合并并整理，形成统一的数据视图
数据清洗：针对原始数据，对出现的噪声进行修复、平滑或者剔除。包括异常值、缺失值、重复记录、错误记录等；同时过滤掉不用的数据，包括某些行或某些列。
数据变换：对数据进行变换处理，使数据更适合当前任务或者算法的需要。
数据规约：在尽可能保持数据原貌的前提下，最大限度地精简数据量。主要包括属性选择和数据抽样两种方法。
5.2 描述性统计分析基础
统计分析的概念：统计分析法就是运用数学方式，建立数学模型，对通过调查获取的各种数据及资料进行数理统计和分析，形成定量的结论。
常见的统计分析方法：均值、标准差、中位数、四分位数、比例、比率、频数、频率、偏度、峰度等。（具体请查询相关文档）【重点】
概率分布：概率是反映随机事件出现的可能性大小。随机事件是指在相同条件下，可能出现也可能不出现的事件。概率分布，是指用于表述随机变量取值的概率规律，比如正态分布。
拟合：是一种把现有数据透过数学方法来代入一条数式的表示方式。比如拟合直线或多项式曲线。
5.3 假设检验
假设检验的概念：根据样本的信息检验关于总体的某个假设是否正确。这类问题称作假设检验问题。基本任务是根据样本所提供的信息，对未知总体分布的某些方面（常见如总体均值、总体方差、总体分布本身等等）的假设做出合理的判断。
正态分布T检验
单样本T检验：是检验一个样本平均数与一个已知的总体平均数的差异是否显著。
双样本T检验：是检验两个样本平均数与其各自所代表的总体的差异是否显著。双样本T检验又分为两种情况：
✓一是独立样本T检验：独立样本T检验就是根据样本数据对两个样本来自的两个独立总体的均值是否有显著差异进行推断；进行独立样本T检验的条件是，两个样本的总体相互独立且符合正态分布。
✓配对样本T检验配：对样本是指对同一样本进行两次测试所获得的两组数据，或对两个完全的样本在不同条件下进行测试所得到的两组数据；配对样本T检验的前提条件：两样本是配对的（数量一样，顺序不能变），服从正态分布。
T检验的前提：
✓来自正态分布总体
✓随机样本
✓均数比较时，要求俩总体方差相等，即具有方差齐性
4、二项分布检验：二项分布检验是通过样本数据检验样本来自的总体是否服从指定的概率为P的二项分布，其原假设是：样本来自的总体与指定的二项分布无显著差异。
5.4 数据指标
1、数据分析指标基础知识
指标（Indicator）：指预期中打算达到的指数、规格、标准。
指标的组成要素：【重点】
✓指标名称，说明所反映现象数量特征的性质和内容
✓统计的时间界限和空间范围
✓计量单位
✓指标的数值
2、数据指标可以从不同角度分类
定性指标与定量指标：定性指标，通常是非结构化的、经验性的、揭示性的、难以归类的；定量指标是指可以被计数和衡量的指标。比如产量销量、用户数。【重点】
虚荣指标与可行动指标：虚荣指标（表面指标）指表面指标，这部分指标往往较大而泛，可以给人留下印象，但是无法用于相关的决策；可行动指标（明确指标）指运营性的指标，这类指标可以为我们指明工作的方向，帮助我们改进商业模式，决策下一步的行动。
先见性指标与后见性指标：先见性指标可以用于预测企业未来的情况。通过对未来的预测，指定利益最大化的策略。后见性指标用于揭示当前存在的问题。发现存在的问题后通过干预，减少损失。
相关性指标与因果指标：相关性指标是某些存在关联性的指标。因果指标是一个或多个指标的改变（自变量）能够对另一个指标产生影响。
数量指标与质量指标：数量指标反映规模大小和数量多少等数量特征的各种指标，比如总销售额、总人数。质量指标反映经济和社会发展的质量、效率和经济效益的指标,通常采用相对数或平均数表示。比如销售业务达标率。【重点】
3、指标衡量标准：
可比较：好的数据指标是比较性的。比如能比较某数据指标在不同的时间段、用户群体、竞争产品之间的表现。
可理解：好的数据指标是简单易懂的。如果不能很容易地记住或讨论某指标，那么通过改变它来改变公司的作为会十分困难。
可指导：好的数据指标会改善行为，与目标保持一致。
好的数据指标是一个比率。比率的可操作性强，是行动的向导，比率是天生的比较性指标。比如某个月新客户增长量。【重点】
4、指标体系：
指标体系是指由一系列相互之间有逻辑联系的指标所组成的整体，从各个侧面反映出现象总体或样本的数量特征。所以一个指标不能叫指标体系，几个毫无关系的指标也不能叫指标体系。【重点】
5.5 数据分析编程
1、数据编程方法
分析用数据表：指将业务主题相关的指标、维度、属性关联在一起的一张数据库表。通常也叫宽表，从字面意义上讲就是字段比较多的数据库表。这种宽表的设计广泛应用于数据分析和挖掘前的数据准备。【重点】
使用分析用数据表的好处：可以大大迭代计算时的效率问题；方便计算报表KPI；易于管理维护分析项目；等等。【重点】
数据分析用表处理方法：工作通常是根据需求寻找数据源，检查数据质量，然后将数据整合成一张分析用数据表。比如要分别统计购买了某商品和没有购买某商品客户的总消费次数，总消费金额，数据用表需要准备客户ID、是否购买了该商品、购买次数、购买金额。再比如现有门店的销售统计数据，要统计门店所在区域的汇总，则可以通过门店与区域映射表在门店表增加区域字段。【重点】
2、数据编程的过程
第一步：理解项目要求。理解项目商业目标或要求，梳理项目目标、分析范围、分析维度、交付形式。比如信用卡公司统计借款不还的用户数据，也有必要理解数据的业务目标，比如业务部门将会如何对待借款不还的客户。再比如销售公司需要根据前两年每个季度KPI的同比结果上升还是下降来决定是否关店，如果只门店上一季度的相关KPI，则没有匹配分析目标和分析范围。【重点】
第二步：确定数据要求。包括项目数据化、查找数据表、数据质量控制。根据分析目的确定分析指标、维度、粒度。比如分析传感器温度，如果分析的目的只需要温度区间就可以满足，可以做离散化处理。【重点】
第三步：数据整合。首先按维度统计各类度量形成临时表，再通过join聚合到分析用数据表，就完成了数据的整合，得到了完整的分析用数据表。此处聚合操作是按维度进行多个属性聚合，维度值不变，因此聚合后字段增加，行数不变，但有些属性字段会出现缺失值。【重点】
3、数据分析编程注意事项
✓数据编程的效率：主要包含2个方面：编程效率和程序运行效率。【重点】
✓编程效率：在工作中扮演着至关重要的角色，优秀的工程师往往都有着很高的效率。可以通过训练编程能力、阅读代码和技术资料、编程工具等方式提高编程效率。
✓编程规范性：企业内部实行统一的编程规范，能带来诸多好处。比如使代码更美观、阅读更方便；使代码的逻辑更清晰、更易于理解；增加程序被复用的机会；有利于程序的管理和维护，等等。【重点】
✓编程质量控制：编程质量控制包括多个方面，比如运行的效率、数据的质量、程序的规范性、程序语法错误、逻辑错误等等。其中非常重要是要保证分析结果的准确性，这是项目提出方的业务部门的主要质量评估标准，也是数据分析师最应关注的。【重点】
5.6 数据分析报告
1、数据分析报告的作用：
展示分析结果、验证分析质量、针对问题提出优化措施或解决策略。
2、数据分析报告的原则：
规范性：数据报告要“以数据说话”。所使用数据单位、名词术语一定要标准统一，前后一致,基本上要与前人所提出的相一致。所使用指标的数据来源要有清晰的说明，从数据管理系统采集的，要说明系统名称。现场测量的要说明抽样方式、抽样量和测量时间段等。
严谨性：数据分析报告的编制过程一定要谨慎，体现在：基础数据须要真实完整；分析过
程须要科学合理全面；分析结果可靠，建议内容实事求是。
重要性：数据分析报告的输出是整个分析过程的成果，是评定一个产品、一个运营事件的定性结论，很可能是产品决策的参考依据。因此数据分析报告一定要体现项目分析的重点，在项目各项数据分析中，就应该重点选取真实性、合法性指标，构建相关模型，科学专业地进行分析，并且反映在分析结果中对同一类问题的描述中，也要按照问题的重要性来排序。
创新性：科技发展、进步，创新的方法或模型从实践中摸索总结出来，数据分析报告要将这些创新的想法记录下来，发扬光大。
3、数据分析报告的结构：
一般数据分析报告包括目录、分析目的与背景、分析结论、正文、建议、附录等几部分。其中附录根据分析报告的需要选择性地添加，其他一般必不可少。比如缺少改进建议，就是一份不完整的报告。【重点】
第6章  数据质量
6.1 数据质量的概念和评价维度
1. 数据质量的概念
   数据质量说的是数据的实际状态与期望状态的比较。
2. 数据质量的评价维度【重点】
   规范性：数据符合其定义的语法（格式，类型）；度量数据是否符合数据库、元数据或文档规则所规定的类型（字符串，整数，浮点等），格式（长度，位数等）
   唯一性：根据事物被识别的方式，没有任何一个事物存在多次记录；度量数据或者数据的属性是否是重复的。数据库中，重复可能存在于单个表中，也可能存在于跨表关系中。
   准确性：数据正确描述现实的程度；度量数据的值是否是准确的。准确的含义是指数据和它所表示的实体实际值的接近程度，越接近准确度越高。
   完整性：元组（理解为表中的记录）的属性值或者元组本身的完整程度；度量数据是否完整，包含两方面，一种是属性值缺失即CWA（Closed World Assumption），一种是元组缺失即OWA（Opened World Assumption）
   时效性：数据在所需时间点描述现实的程度；度量哪些数据过期不可用，哪些数据仍在有效期。对于失效的数据需要及时更新状态或者清理。【重点】
   6.2 数据质量管理的标准
   考察数据质量的标准：
   数据完整性，是指对非空值（null 或空字符串）的度量，任何数据项、记录、数据集或数据库中，需要评估的关键数据；适用范围：任何数据项、记录、数据集或数据库中，需要评估的关键数据；相关维度：有效性、准确性
   数据唯一性，是指任何事物都不会被记录超过一次，是对现实世界中的事物数量与数据集中的事物记录数量的比较；适用范围：单个数据集中的所有记录；相关维度是一致性
   数据及时性，是指数据在所需要的时间点反映现实的程度，是时间差的度量，适用范围是任何数据项、记录、数据集或数据库；相关维度是准确性
   数据有效性，是指如果数据符合定义的语法（格式、类型、范围），则它是有效的；是数据与元数据或文档之间的比较；适用范围：所有数据通常都可以进行有效性测量；相关维度是准确性、完整性、一致性和唯一性
   数据准确性，是指数据正确描述“真实世界”对象或事件的程度，是数据在多大程度上反映了真实对象的特征的度量；适用范围是任何数据项、记录、数据集或数据库中保存的对象或事件；相关维度是有效性
   数据一致性，是同一个事物的多条记录之间是否有区别，是模式分析的度量；适用范围是多个数据集，评估事物的数值或格式；相关维度是有效性、准确性和唯一性【重点】

数据质量的相对性：
考察数数据质量除了上述几个维度问题外，还需要结合具体的业务情况来进行综合考量，数据质量是相对的；在数据分析中数据质量是由分析需求决定的，有时不需要完整数据也能分析如通常情况下，异常值是数据质量问题，但有时候异常值可能是合理合法的。【重点】
6.3 数据质量问题的处理
1. 重复值处理：【重点】
   重复数值产生的原因：
   数据源重复，违背唯一性原则
   提取逻辑问题（多表关联时出现一对多、多对多、多对一关联，结果必然会有重复）对应处理：
   常用SQL清洗技术：distinct 、groupby 、row number() ；
   根据主键或者其他独特的属性，进行匹配处理
   采用自定义函数、业务逻辑限制处理
   通过算法计算相似度处理
   数据源系统规范设计、数据库技术
2. 缺失数值问题：
   常见原因：源数据遗漏、业务信息缺失、数据提取应用错误等
   常见处理：
   在SQL中，缺失的数据通常以Null或空字符串表示
   识别缺失可以通过数据统计中的记录值和唯一值进行评估，包括探查维表内是否包含事实表所有主键
   处理缺失值常用方法常见有删除、填充（某常量填充、某统计值填充、拟合值填充等）
3. 异常值发现与修复
   常见原因：业务问题、数据突变、错误数据
   检测异常数值：简单统计分析、3σ原则、箱型图分析等
   异常值处理：删除、替换、其他算法处理
   6.4 数据质量带来的影响
   业务影响：如库存不足或者过多、结算错误、财务计划的偏差、信息不及时或不准确等【重点】
   企业损失：收入损失、成本增加、客户投诉、市场占有率降低【重点】
   6.5 数据质量控制
   数据质量控制需要技术、管理、人才三者配合，要想真正长期保证数据的高质量，还必须从以下5个方面着手：【重点】
   建立数据的标准，明确数据的定义（权衡企业内外部因素，即习惯和质量）。
   建立一个可复用的数据收集、数据预处理和数据维护流程（应对不断变化的企业内外部因素）。（说明：可以基于数据处理平台，如数据仓库来实现）
   在数据预处理流程中设立多个性能监控点（评价标准：最终用户、同类数据、前期数据等）。（说明：可以基于数据质量管理平台来实现）
   对流程不断进行改善和优化（质量改善非一朝一夕，而是持续过程，要灵活变通）。
   把责任落实到人（制定数据采集、存储、集成、分析等各项活动的质量指标）。
   第7章  数据项目设计与执行
   7.1 项目实施过程知识
1. 项目管理（Project Management）：
   运用各种相关技能、方法与工具，为满足或超越项目有关各方对项目的要求与期望，所开展的
   各种计划、组织、领导、控制等方面的活动。
2. 软件（IT）项目管理一般涉及如下几个阶段【重点】
   阶段1计划和需求分析(Planning and Requirement Analysis)
   阶段2设计项目架构(Project Architecture)
   阶段3开发和编程(Development and Coding)
   阶段4测试(Testing)
   阶段5部署(Deployment)
3. 数据分析项目实施过程：
   明确目的、数据收集、数据处理、数据分析、数据展现、分析报告
4. 项目里程碑：
   设定里程碑的目的是能够很好的让所有干系人了解到项目进展到哪一个阶段了；IT项目可以分为设计、开发、单元测试、集成测试、（部署）发布等。选取每个任务的完成点设为里程碑。在每一个里程碑的时候，都要有“完成”的交付物。【重点】
5. 项目通气会
   是让项目所有干系人对项目的整体情况作介绍，收集信息，在项目实施过程随时可以进行。【重点】
6. 项目前评估
   是全部项目评估中最重要的一个部分。广义的项目前评估是指在项目前期决策阶阶段，从整个项目全局出发，根据企业发展的需要对项目及其被选方案所进行的全面评估，从而辨别项目及其被选方案的可行和优劣，决定取舍；即对项目的必要性和项目备选方案的技术、经济、运行条件和社会与环境影响等方面所进行的全面论证与评估的工作。具体对数据分析项目来讲就是评估该项目开展原因、项目实施过程中的成本估算、项目实施中的风险及收益预测。【重点】
7. 在实施数据分析中的经常型(Routine Job)项目时
   如果希望系统按预先设定的时间自动运行程序，编程人员需要考虑程序处理异常情况的能力、报错的警告如何通知到责任人、输出结果的质检报告如何设计和日常维护是否便利等。对比临时性项目，经常型项目更关注程序输出结果的自动化程度【重点】

7.2 数据分析项目的目标
项目目标。项目目标（Project Objectives）：简单地说就是实施项目所要达到的期望结果，即项目所能交付的成果或服务；项目的考核绩效必然与项目的目标一致。
数据分析项目目标的确定过程：
项目情况分析
问题界定
确定目标因素
建立目标体系
各目标关系确认【重点】
数据分析项目目标的制定原则Smart原则【重点】
目标必须是具体的（Specific）
目标必须是可以衡量的（Measurable）
目标必须是可以达到的（Attainable）
目标必须和其他目标具在相关性（Relevant）
目标必须具在明确的截止期限（Time-based）
7.3 数据分析中的客户忠诚度
客户忠诚度，又可称为客户粘度，是指客户对某一特定产品或服务产生了好感，形成了“依附性”偏好，进而重复购买的一种趋向。客户忠诚营销理论的关心点是利润。【重点】

顾客生命价值（Customer Lifetime Value，CLV）是企业从每个顾客在未流失前的所有交易活动中得到的净收入。通常使用其生命周期内每个相关时期的收入、长期附加产品和服务的购买额、长时期由他介绍来的其他顾客数量等来衡量。【重点】
第8章  机器学习（数据挖掘）
8.1 常见的算法分类
数据挖掘的基本任务主要体现在关联规则、分类与回归、聚类等几个方面：【重点】
总体上分为有监督学习（预测性任务）和无监督学习（描述性任务）
有监督学习包括分类和回归；无监督学习包括聚类和关联分析
预测性和描述性的主要区别在于是否有目标变量
8.2 机器学习的算法评估
分类问题，常用混淆矩阵，计算指标准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1 score、ROC曲线、AUC来判定；【重点】
回归问题，常用平均绝对误差（MAE）、平均平方误差（MSE）、均方根误差（RMSE）等
聚类问题，兰德指数、互信息等【重点】
8.3 聚类分析
聚类分析是对具有共同趋势或结构的数据进行分组，将数据项分组成多个簇（类），簇之间的数据差别尽可能大，簇内的数据差别尽可能小，即“最小化”簇间的相似性，最大化簇间的相似性。它是典型的无监督分析方法，也就是没有关于样品或变量的分类标签，分类需要依据样品或者变量的亲疏程度进行。【重点】
常见算法K-means ：K均值聚类（K-means）算法是无监督聚类算法中的代表，该算法的主要作用是将相似的样本自动归到一个类别中。【重点】
操作步骤：【重点】
首先设定K值，即确定聚类数；确定各类中心；
计算每个记录到类中心的距离，并将该记录归到最近的类中；
然后重新计算K类的中心点，更新原类族的中心；
重复第二、三步，迭代到收敛标准停止。

K-means 是基于划分的聚类，不适宜于在非球形的群体中识别类别；基于密度的聚类可以获得任意形状的簇，如DBSCAN、OPTICS和DENCLUE等

8.4 分类算法
分类作为一种监督学习方法，它的目标在于通过已有数据的确定类别，学习得到一个分类函数或分类模型(也常常称作分类器)，该模型能把数据库中的数据项映射到给定类别中的某一个类中。【重点】
决策树
概念：根据一些feature（特征）进行分类，每个节点提一个问题，通过判断，将数据分为两类，再继续提问。这些问题是根据已有数据学习出来的，再投入新数据的时候，就可以根据这棵树上的问题，将数据划分到合适的叶子上；【重点】
ID3算法是一种经典的实现决策树的算法。ID3算法使用信息增益作为属性选择标准。信息增益是数据划分前后的熵的差值，ID3算法采用使得信息增益最大的特征来划分当前的数据。【重点】
KNN算法
概念：k近邻（k-NearestNeighbor，kNN）分类算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中；

K数值的选择：【重点】
K取值过小，整体模型变复杂，容易发生过拟合，容易对异常值敏感
K取值越大，优点是可以减少学习的估计误差，但缺点是学习的近似误差增大
如果K值取样本数N，即K=N，无论输入实例是什么，都将简单的预测他属于训练实例中最多的类，模型最复杂；如果K=1则没什么意义了，就1个类；在实际应用中，K 值一般选择一个较小的数值
8.5 关联分析相关算法
Apriori 算法是一种最有影响的挖掘布尔关联规则频繁项集的算法，所涉及的重要概念支持度、置信度：【重点】
设关联规则：A=>B，{A}或{B}为项集，项集{A}可以认为为前因，项集{B}可以认为为后果。
支持度表示的是两个事件同时发生的概率，也就是表示同时包含A、B事务占总事务的百分比。
置信度是预测性指标，是在前因发生的条件下，后果发生的概率，表示同时发生A、B事务的概率占事务A发生概率的百分比。
支持度为对称指标，即{A,B}的支持度与{B,A}的支持度都一样，
而置信度为非对称指标，二者不同，即置信度(A=>B)  与置信度(B=>A) 不一样。
支持度高置信度不一定高【重点】
8.6 回归分析
1、回归定义：【重点】
回归是处理变量之间关系的一种统计方法，主要用来处理数值型自变量和数值型因变量之间的关系。自变量又称为解释变量具有确定性，因变量也称为预报变量，具有随机性
2、回归的分类：
根据因变量和自变量的个数来分类：一元回归分析，多元回归分析
根据因变量和自变量的函数表达式来分类：线性回归分析，非线性回归分析
3、分类与回归的关系：
回归与分类相似，两者都需要构建模型，都用模型来估计未知值
回归不同于分类，分类法主要是用来预测类标号（分类属性值），回归法主要是用来估计连续值（量化属性值）
4、常见回归模型
1）一元线性回归：只有一个变量X与因变量Y有关，X与Y都是连续型变量，因变量Y或其残差必须服从正态分布：y=β0+β1x+ε（模型）【重点】
其中，x为自变量，y为因变量。β0和β1称为模型的参数，β0为截距，β1为回归系数，表明自变量对因变量的影响程度。误差项ε是随机变量，反映了除x和y之间的线性关系外的随机因素对y的影响，是不能由x和y之间的线性关系所解释的变异性。求解实质上求解β0和β1，可以使用最小二乘、梯度下降等方法来求解；最小二乘法是通过最小化残差平方和来估计回归系数，最小二乘法的优点：最有解唯一、求解方便、好的解析性质即最小二乘法在正态分布假设下可以用极大似然估计(MLE)解释，也可以证明解是最优线性无偏估计。【重点】
2）逻辑回归：分析多个变量与因变量Y的关系，Y通常是离散型或定性变量，该模型对因变量Y的分布无要求；是一种广义的线性回归分析模型，常用于数据挖掘，疾病自动诊断，经济预测等领域【重点】
说明：逻辑回归，都是以线性回归为理论支持的；对于非线性问题，直接处理是不支持的，可以先做特征线性变换处理，再考虑逻辑回归模型【重点】
大数据概述
大数据问题的处理思路
将问题简化成更简单能处理的问题，问题拆解，分治做法

云计算提供存储与计算的基础设施，大数据是运行在其上的应用

大数据的特征

概括为4V特征：Volume体量巨大、Variety种类繁多、Velocity处理速度快、value价值密度低（即单位体量数据包含的价值相对较低）

云计算与大数据的区别和联系主要有以下几点：
大数据是为了发掘信息价值，而云计算主要是通过互联网管理资源，提供相应的服务。
大数据的价值在于发掘数据的有效信息，云计算则可以大量节约使用存储成本。
云计算为大数据应用提供基础的硬件设施

Hadoop的特点（4高1低）


HDFS文件系统
HDFS是Hadoop的抽象文件系统，是Hadoop Distribute File System，分布式文件系统，是hadoop体系中数据存储管理的基础，核心存储框架。

MapReduce
mapreduce是一种计算模型，用于处理大数据量的计算。它借助于函数式程序设计语言Lisp的设计思想，提供了一种简便的并行程序设计方法。用Map（映射）和Reduce（归约）两个函数编程实现基本的并行计算任务。先进行Map运算，得出Key-Value的中间结果，然后Reduce对中间结果中相同的键的所有值进行归约，以得到最终
结果。

Hive
Hive是基于Hadoop的一个数据仓库工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。
数据仓库工具。hive定义了一种类似sql的查询语言（hql）将sql转化为mapreduce任务在hadoop上执行。

Hbase
hbase是一个针对结构化数据的可伸缩，高可靠，高性能，分布式和面向列的动态模式数据库。hbase采用了bigtable的数据模型。Hbase是一个高可靠，高性能，面向列，可伸缩的分布式数据库。

Yarn
是一种新的Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度。

Storm
storm是一个分布式的，容错的计算系统，storm属于流处理平台。

Zookeeper
ZooKeeper是一个分布式服务框架，是Apache Hadoop的一个子项目，主要是用来解决分布式应用中遇到的一些数据管理问题

Spark

大数据分析
数据分析过程
数据分析一般包括如下过程：明确目的、数据收集、数据处理、数据分析、数据展现、分析报
告。
• 明确目的：明确数据分析的目标、数据指标，以及要达到的效果
• 数据收集：确定数据范围、获取目标数据、整合相关数据。不同阶段根据需求采集所需数
据范围和数据类型，同时要考虑数据质量。数据源多了，往往需要对不同的数据源做不同
的数据预处理，有时候数据多样性还会降低数据质量。
• 数据处理：包括处理缺失数据、清洗不一致数据、关联、汇总数据
• 数据分析：通过SQL、编程等实现数据分析
• 数据展现：将数据分析结果以图表、报表形式直观展现
• 分析报告：简单可靠、清晰明了、逻辑性强的数据分析报告

数据分析方法分类

大数据采集方法

大数据预处理技术——数据清洗

大数据的存储

大数据的计算


数据库
数据库设计的三范式
原子性、唯一性、独立性（减少数据冗余）
数据库基本知识
传统数据库系统分成网状数据库（Network database）、层次数据库（Hierarchicaldatabase）和关系数据库（Relational database）三类。

关系型数据库

关系模型的基本概念

数据仓库基本知识
数据仓库（Data Warehouse）是一个面向主题的（Subject Oriented）、集成的（Integrated）、非易失的（Non-Volatile）、反映历史变化（Time Variant）的数据集合，用于支持管理决策(Decision Making Support)。



机器学习
机器学习方法整体流程


SQL
SQL简介
SQL分类DDL、DML、DQL（查询）

SQL综述
1. DDL
   CREATE
   ALTER
   DROP
   TRUNCATE
   COMMENT
   RENAME
2. DML
   insert into
   delete
   update


错题集合
